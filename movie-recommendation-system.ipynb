{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1187,"sourceType":"datasetVersion","datasetId":626}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================================\n# MovieLens 100K — UserCF vs ItemCF vs SVD\n# Path: /kaggle/input/movielens-100k-dataset/ml-100k\n# Evaluate with Precision@K\n# =========================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\n\n# --------------------------\n# 1) Load ratings & movies\n# --------------------------\nRATINGS_PATH = \"/kaggle/input/movielens-100k-dataset/ml-100k/u.data\"\nMOVIES_PATH  = \"/kaggle/input/movielens-100k-dataset/ml-100k/u.item\"\n\n# u.data: userId, movieId, rating, timestamp (tab-separated)\nratings = pd.read_csv(\n    RATINGS_PATH, sep=\"\\t\",\n    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n    engine=\"python\"\n)\nratings = ratings.drop(columns=\"timestamp\")\n\n# u.item: pipe-separated with many columns; first two are movieId, title\nmovies_raw = pd.read_csv(\n    MOVIES_PATH, sep=\"|\", header=None, encoding=\"latin-1\", engine=\"python\"\n)\nmovies = movies_raw[[0, 1]].copy()\nmovies.columns = [\"movieId\", \"title\"]\nmovies[\"movieId\"] = movies[\"movieId\"].astype(int)\nmovie_title = dict(zip(movies.movieId, movies.title))\n\nprint(\"Ratings shape:\", ratings.shape, \"Unique users:\", ratings.userId.nunique(), \"Unique movies:\", ratings.movieId.nunique())\nprint(ratings.head(), \"\\n\")\nprint(movies.head(), \"\\n\")\n\n# --------------------------\n# 2) Train/Test split (by interactions)\n#    Ensure test users exist in train\n# --------------------------\ntrain, test = train_test_split(ratings, test_size=0.2, random_state=42)\nusers_in_train = set(train.userId.unique())\norphan_mask = ~test.userId.isin(users_in_train)\nif orphan_mask.any():\n    # move orphan test rows back to train\n    train = pd.concat([train, test[orphan_mask]], ignore_index=True)\n    test = test[~orphan_mask].reset_index(drop=True)\n\n# --------------------------\n# 3) Build user-item matrices (TRAIN)\n# --------------------------\n# Use movieId as columns (safer), map to titles only for display.\nuser_item_train = train.pivot_table(index=\"userId\", columns=\"movieId\", values=\"rating\")\nuser_item_train_filled = user_item_train.fillna(0.0)\n\nusers = user_item_train.index.tolist()\nitems = user_item_train.columns.tolist()\nuser_index = {u: i for i, u in enumerate(users)}\nitem_index = {m: j for j, m in enumerate(items)}\n\n# Helper: items rated in train by a user\ndef seen_items_train(uid):\n    if uid not in user_item_train.index:\n        return set()\n    return set(user_item_train.loc[uid].dropna().index.tolist())\n\n# --------------------------\n# 4) USER-BASED CF\n# --------------------------\nuser_sim = cosine_similarity(user_item_train_filled.values)  # (U x U)\nuser_sim_df = pd.DataFrame(user_sim, index=users, columns=users)\n\ndef recommend_usercf(uid, top_k=10, n_neighbors=50, min_common=3):\n    if uid not in users:\n        return []\n    # Top similar neighbors\n    sims = user_sim_df.loc[uid].drop(uid, errors=\"ignore\").sort_values(ascending=False).head(n_neighbors)\n    neighbors = sims.index.tolist()\n    weights = sims.values\n\n    # Neighbor ratings matrix (neighbors x items)\n    neigh_mat = user_item_train.loc[neighbors, items]\n    # Mask of where neighbors rated\n    rated_mask = ~neigh_mat.isna()\n\n    # Weighted numerator & denominator\n    ratings_filled = neigh_mat.fillna(0.0).values\n    numer = (ratings_filled * weights.reshape(-1, 1)).sum(axis=0)\n    denom = (rated_mask.values * weights.reshape(-1, 1)).sum(axis=0)\n\n    # Scores\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        scores = np.where(denom > 0, numer / denom, 0.0)\n    scores_s = pd.Series(scores, index=items)\n\n    # Require at least min_common neighbor ratings per item\n    common_counts = pd.Series(rated_mask.values.sum(axis=0), index=items)\n    scores_s = scores_s[common_counts >= min_common]\n\n    # Exclude seen items in TRAIN\n    scores_s = scores_s.drop(labels=list(seen_items_train(uid)), errors=\"ignore\")\n\n    # Top-K itemIds\n    recs = scores_s.sort_values(ascending=False).head(top_k).index.tolist()\n    return recs\n\n# --------------------------\n# 5) ITEM-BASED CF\n# --------------------------\n# Compute item similarity on TRAIN\nitem_item_matrix = user_item_train_filled.T  # (I x U)\nitem_sim = cosine_similarity(item_item_matrix.values)  # (I x I)\nitem_sim_df = pd.DataFrame(item_sim, index=items, columns=items)\n\ndef recommend_itemcf(uid, top_k=10, n_neighbors=50, min_common=3):\n    if uid not in users:\n        return []\n    user_ratings = user_item_train.loc[uid]  # sparse (with NaN)\n    seen = user_ratings.dropna()\n\n    # Weighted score for an unseen item j:\n    # sum_{i in seen top-n similar} sim(j,i) * r(u,i) / sum sim(j,i)\n    scores = {}\n    for j in items:\n        if j in seen.index:\n            continue\n        sims = item_sim_df.loc[j, seen.index].copy()\n        # Keep top-N most similar rated items\n        sims = sims.sort_values(ascending=False).head(n_neighbors)\n        sim_vals = sims.values\n        rat_vals = seen.loc[sims.index].values\n        # Count common (non-zero sims)\n        common = (sim_vals > 0).sum()\n        if common < min_common or sim_vals.sum() <= 0:\n            continue\n        scores[j] = np.dot(sim_vals, rat_vals) / sim_vals.sum()\n\n    # Top-K itemIds\n    recs = [m for m, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]]\n    return recs\n\n# --------------------------\n# 6) SVD (Matrix Factorization)\n# --------------------------\n# Center by user mean to reduce bias, reconstruct scores, then uncenter.\nR = user_item_train_filled.values  # zeros for missing\n# Compute user means on observed ratings only\nuser_means = user_item_train.apply(lambda row: row.mean(), axis=1).fillna(0.0).values\nR_centered = user_item_train.subtract(user_means, axis=0).fillna(0.0).values\n\n# Low-rank approximation with TruncatedSVD\nrank = 50  # you can tune (e.g., 20–100)\nsvd = TruncatedSVD(n_components=rank, random_state=42)\nU = svd.fit_transform(R_centered)          # (U x k)\nS = svd.singular_values_                   # (k,)\nVT = svd.components_                       # (k x I)\n\n# Reconstruct centered scores and add user means back\nR_hat_centered = U @ np.diag(S) @ VT       # (U x I)\nR_hat = (R_hat_centered + user_means.reshape(-1, 1))  # add back means\n\ndef recommend_svd(uid, top_k=10):\n    if uid not in users:\n        return []\n    uidx = user_index[uid]\n    scores = pd.Series(R_hat[uidx, :], index=items)\n    scores = scores.drop(labels=list(seen_items_train(uid)), errors=\"ignore\")\n    recs = scores.sort_values(ascending=False).head(top_k).index.tolist()\n    return recs\n\n# --------------------------\n# 7) Evaluation: Precision@K\n# --------------------------\ndef precision_at_k(method_func, uid, k=10, relevance_threshold=4.0):\n    recs = method_func(uid, top_k=k)\n    if not recs:\n        return 0.0\n    # Relevant items in TEST: rating >= threshold\n    user_test = test[test.userId == uid]\n    if user_test.empty:\n        return 0.0\n    relevant = set(user_test.loc[user_test.rating >= relevance_threshold, \"movieId\"].tolist())\n    if not relevant:\n        return 0.0\n    hits = sum(1 for m in recs if m in relevant)\n    return hits / k\n\ndef evaluate_all(users_subset=None, k=10):\n    if users_subset is None:\n        users_subset = users\n    metrics = {\"UserCF\": [], \"ItemCF\": [], \"SVD\": []}\n    for uid in users_subset:\n        metrics[\"UserCF\"].append(precision_at_k(recommend_usercf, uid, k=k))\n        metrics[\"ItemCF\"].append(precision_at_k(recommend_itemcf, uid, k=k))\n        metrics[\"SVD\"].append(precision_at_k(recommend_svd, uid, k=k))\n    summary = {\n        \"UserCF_P@{}\".format(k): np.mean(metrics[\"UserCF\"]) if metrics[\"UserCF\"] else 0.0,\n        \"ItemCF_P@{}\".format(k): np.mean(metrics[\"ItemCF\"]) if metrics[\"ItemCF\"] else 0.0,\n        \"SVD_P@{}\".format(k): np.mean(metrics[\"SVD\"]) if metrics[\"SVD\"] else 0.0,\n        \"Users_Evaluated\": len(users_subset),\n    }\n    return summary\n\n# --------------------------\n# 8) Run: sample recommendations & overall metrics\n# --------------------------\nK = 10\nsample_user = users[0]\n\nprint(\"Sample user:\", sample_user)\nprint(\"\\nUserCF recommendations:\")\nfor mid in recommend_usercf(sample_user, top_k=10):\n    print(\"-\", movie_title.get(mid, str(mid)))\n\nprint(\"\\nItemCF recommendations:\")\nfor mid in recommend_itemcf(sample_user, top_k=10):\n    print(\"-\", movie_title.get(mid, str(mid)))\n\nprint(\"\\nSVD recommendations:\")\nfor mid in recommend_svd(sample_user, top_k=10):\n    print(\"-\", movie_title.get(mid, str(mid)))\n\n# Evaluate on all users (can be slow; you can subsample)\nresults = evaluate_all(users_subset=users, k=K)\nprint(\"\\n=== Precision@{} (higher is better) ===\".format(K))\nfor k_, v in results.items():\n    print(f\"{k_}: {v:.4f}\" if isinstance(v, float) else f\"{k_}: {v}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:29:19.222151Z","iopub.execute_input":"2025-08-15T15:29:19.222858Z","iopub.status.idle":"2025-08-15T15:43:56.112700Z","shell.execute_reply.started":"2025-08-15T15:29:19.222827Z","shell.execute_reply":"2025-08-15T15:43:56.111331Z"}},"outputs":[{"name":"stdout","text":"Ratings shape: (100000, 3) Unique users: 943 Unique movies: 1682\n   userId  movieId  rating\n0     196      242       3\n1     186      302       3\n2      22      377       1\n3     244       51       2\n4     166      346       1 \n\n   movieId              title\n0        1   Toy Story (1995)\n1        2   GoldenEye (1995)\n2        3  Four Rooms (1995)\n3        4  Get Shorty (1995)\n4        5     Copycat (1995) \n\nSample user: 1\n\nUserCF recommendations:\n- Braindead (1992)\n- Touch of Evil (1958)\n- Waiting for Guffman (1996)\n- Down by Law (1986)\n- Titanic (1997)\n- Secrets & Lies (1996)\n- Casablanca (1942)\n- Some Folks Call It a Sling Blade (1993)\n- Shawshank Redemption, The (1994)\n- Close Shave, A (1995)\n\nItemCF recommendations:\n- Further Gesture, A (1996)\n- Burnt By the Sun (1994)\n- Looking for Richard (1996)\n- C'est arrivé près de chez vous (1992)\n- Wings of the Dove, The (1997)\n- Strictly Ballroom (1992)\n- Celluloid Closet, The (1995)\n- Go Fish (1994)\n- Crumb (1994)\n- Sweet Hereafter, The (1997)\n\nSVD recommendations:\n- Raising Arizona (1987)\n- Schindler's List (1993)\n- Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)\n- Rear Window (1954)\n- Back to the Future (1985)\n- Lawrence of Arabia (1962)\n- Fargo (1996)\n- Secrets & Lies (1996)\n- Shawshank Redemption, The (1994)\n- Reservoir Dogs (1992)\n\n=== Precision@10 (higher is better) ===\nUserCF_P@10: 0.0575\nItemCF_P@10: 0.0269\nSVD_P@10: 0.1699\nUsers_Evaluated: 943\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}